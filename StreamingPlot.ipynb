{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, substring, from_json\n",
    "from decimal import Decimal\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StreamingPlot\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "tickData_schema = StructType([\n",
    "    StructField('timestamp', StringType(), True),\n",
    "    StructField('symbol', StringType(), True),\n",
    "    StructField('side', StringType(), True),\n",
    "    StructField('size', StringType(), True),\n",
    "    StructField('price', StringType(), True),\n",
    "    StructField('tickDirection', StringType(), True),\n",
    "    StructField('trdMatchID', StringType(), True),\n",
    "    StructField('grossValue', StringType(), True),\n",
    "    StructField('homeNotional', StringType(), True),\n",
    "    StructField('foreignNotional', StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.readStream.option(\"header\",\"false\").schema(tickData_schema).csv(\"./output/tick_data/destination\")\n",
    "\n",
    "df_raw.createOrReplaceTempView(\"trade_history\")\n",
    "\n",
    "aggInterval = \"5 minutes\"\n",
    "\n",
    "def createSQLStatement(interval):\n",
    "    switcher = {\n",
    "        \"15 seconds\" : \",cast(concat(date(timestamp),' ',hour(timestamp),':',minute(timestamp),':',floor(second(timestamp)/15)*15) as timestamp) as group_column\",\n",
    "        \"30 seconds\" : \",cast(concat(date(timestamp),' ',hour(timestamp),':',minute(timestamp),':',floor(second(timestamp)/30)*30) as timestamp) as group_column\",\n",
    "        \"1 minutes\"  : \",cast(concat(date(timestamp),' ',hour(timestamp),':',minute(timestamp)) as timestamp) as group_column\",\n",
    "        \"5 minutes\"  : \",cast(concat(date(timestamp),' ',hour(timestamp),':',floor(minute(timestamp)/5)*5) as timestamp) as group_column\",\n",
    "        \"15 minutes\" : \",cast(concat(date(timestamp),' ',hour(timestamp),':',floor(minute(timestamp)/15)*15) as timestamp) as group_column\",\n",
    "        \"1 hours\"  : \",cast(concat(date(timestamp),' ',hour(timestamp),':00') as timestamp) as group_column\",\n",
    "        \"4 hours\"  : \",cast(concat(date(timestamp),' ',floor(hour(timestamp)/4)*4,':00') as timestamp) as group_column\",\n",
    "        \"6 hours\"  : \",cast(concat(date(timestamp),' ',floor(hour(timestamp)/6)*6,':00') as timestamp) as group_column\",\n",
    "        \"12 hours\" : \",cast(concat(date(timestamp),' ',floor(hour(timestamp)/12)*12,':00') as timestamp) as group_column\",\n",
    "        \"1 days\"  : \",cast(concat(date(timestamp),' ','00:00') as timestamp) as group_column\"\n",
    "    }\n",
    "    return switcher.get(interval, \"Please input correct interval\")\n",
    "\n",
    "df_group_temp = spark.sql('SELECT timestamp, symbol, price, size '+ \\\n",
    "                             createSQLStatement(aggInterval)+ \\\n",
    "                             ' FROM trade_history')\n",
    "\n",
    "df_group_temp.createOrReplaceTempView(\"hist_group\")\n",
    "df_interval = spark.sql('''SELECT group_column as timestamp,\n",
    "                           symbol,\n",
    "                           first(price) as open, \n",
    "                           max(price) as high, \n",
    "                           min(price) as low, \n",
    "                           last(price) as close,\n",
    "                           sum(size) as volumn\n",
    "                           FROM hist_group\n",
    "                           group by symbol, group_column\n",
    "                           ''')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_interval.writeStream.queryName(\"interval_data\").outputMode(\"complete\").format(\"memory\").start()\n",
    "\n",
    "query = df_raw.groupBy(\"symbol\").count().writeStream.queryName(\"tick_count\").outputMode(\"complete\").format(\"memory\").start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|symbol|count|\n",
      "+------+-----+\n",
      "|XBTU20| 5237|\n",
      "|XBTM20| 2589|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tick_count = spark.sql( 'Select symbol, count from tick_count' )\n",
    "\n",
    "tick_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- XBTU20_Close: string (nullable = true)\n",
      " |-- XBTM20_Close: string (nullable = true)\n",
      "\n",
      "+-------------------+------------+------------+\n",
      "|          timestamp|XBTU20_Close|XBTM20_Close|\n",
      "+-------------------+------------+------------+\n",
      "|2020-06-17 03:35:00|      9511.5|      9481.0|\n",
      "|2020-06-17 03:40:00|      9510.0|      9473.0|\n",
      "|2020-06-17 03:45:00|      9514.5|      9476.5|\n",
      "|2020-06-17 03:50:00|      9509.5|      9472.5|\n",
      "|2020-06-17 03:55:00|      9507.0|      9475.0|\n",
      "|2020-06-17 04:00:00|      9502.0|      9469.5|\n",
      "|2020-06-17 04:05:00|      9516.5|      9477.0|\n",
      "|2020-06-17 04:10:00|      9517.5|      9481.0|\n",
      "|2020-06-17 04:15:00|      9516.5|      9480.5|\n",
      "|2020-06-17 04:20:00|      9521.0|      9483.5|\n",
      "|2020-06-17 04:25:00|      9515.0|      9477.0|\n",
      "|2020-06-17 04:30:00|      9510.5|      9474.0|\n",
      "|2020-06-17 04:35:00|      9513.5|      9487.0|\n",
      "|2020-06-17 04:40:00|      9521.5|      9485.5|\n",
      "|2020-06-17 04:45:00|      9526.5|      9486.5|\n",
      "|2020-06-17 04:50:00|      9527.0|      9484.0|\n",
      "|2020-06-17 04:55:00|      9512.0|      9481.0|\n",
      "|2020-06-17 05:00:00|      9515.0|      9479.5|\n",
      "|2020-06-17 05:05:00|      9529.5|      9496.0|\n",
      "|2020-06-17 05:10:00|      9527.5|      9494.0|\n",
      "+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import *\n",
    "\n",
    "df_XBTU20 = spark.sql('''Select timestamp, cast(timestamp as string) as ts_str, close as XBTU20_Close \n",
    "                            from interval_data where symbol = 'XBTU20'\n",
    "                            ''')\n",
    "\n",
    "df_XBTM20 = spark.sql('''Select cast(timestamp as string) as ts_str, close as XBTM20_Close \n",
    "                            from interval_data where symbol = 'XBTM20'\n",
    "                            ''')\n",
    "\n",
    "df_agg = df_XBTU20.join(df_XBTM20, df_XBTU20.ts_str == df_XBTM20.ts_str, 'inner'). \\\n",
    "                        orderBy('timestamp').drop(df_XBTU20.ts_str).drop(df_XBTM20.ts_str)\n",
    "df_agg.printSchema()\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''count = 0\n",
    "while count < 10:\n",
    "    tick_count = spark.sql( 'Select symbol, count from tick_count' )\n",
    "    pd_tick_count = tick_count.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    sns.set()\n",
    "    plt.figure( figsize = ( 10, 8 ) )\n",
    "    sns.barplot( x=\"count\", y=\"symbol\", data=pd_tick_count)\n",
    "    plt.show()\n",
    "    count = count + 1\n",
    "    time.sleep( 600 )\n",
    "\n",
    "    \n",
    "query.awaitTermination()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
